Назначение документа

Документ фиксирует базовые архитектурные принципы и технологические решения CRM-системы.
Он служит отправной точкой для команд разработки, эксплуатации и бизнес-заказчиков, помогая синхронизировать ожидания относительно сервисов и инфраструктуры.
Материал дополняет обзорные документы (README.md, architecture.md, security-and-access.md) и задаёт общие правила для детальных спецификаций отдельных сервисов. Состав первой поставки и приоритизация следующих этапов фиксируются в [плане поставки](delivery-plan.md), который следует использовать в качестве точки входа для планирования релизов. За обзором взаимодействий и потоков данных обращайтесь к разделу [«Взаимодействия и потоки данных»](architecture.md#2-взаимодействия-и-потоки-данных).

Общие архитектурные принципы

Микросервисный подход. Функциональные домены выделяются в самостоятельные сервисы с чёткими API-контрактами, управляемыми через Gateway или BFF (подробнее — [раздел 2.4 архитектуры](architecture.md#24-роль-gatewaybff)).

Единая база данных. Сервисы делят общий PostgreSQL-кластер, изолируясь на уровне схем, ролей и политик доступа; при существенном росте нагрузки возможен поэтапный переход к выделенным кластерам для отдельных доменов (см. [описание кластера](architecture.md#23-единый-postgresql-кластер)).

Асинхронные коммуникации. Для обмена событиями и фоновых задач применяются очереди сообщений (RabbitMQ) и публикация доменных событий; композицию потоков см. в [разделе 2.2 архитектуры](architecture.md#22-асинхронная-шина-rabbitmq).

Кеширование. Для ускорения чтения справочных и агрегированных данных используется Redis или аналогичный in-memory кеш с TTL и контролем согласованности.

Наблюдаемость и устойчивость. Все сервисы обязаны вести структурированные логи, метрики и трассировки (Prometheus, Grafana, OpenTelemetry), обеспечивать алерты и обработку ошибок с повторными попытками.

Управление конфигурацией и секретами. Конфигурации хранятся централизованно (Vault/SSM/Secrets Manager), секреты не попадают в репозиторий, развёртывание автоматизируется через CI/CD.

Безопасность по умолчанию. Реализованы единые механизмы аутентификации/авторизации, шифрование трафика, контроль доступа и регулярные проверки на соответствие требованиям безопасности.

Инфраструктура

Расширенный пример переменных окружения для локальной разработки и CI/CD собран в файле [`env.example`](../env.example). Он отражает секции «Базы данных», «Брокеры сообщений и кэши» и «Service discovery», а также интеграции (серверное файловое хранилище, Telegram), описанные ниже.

Политика ведения seed-миграций и владение справочниками зафиксированы в разделе [«Ведение seed-миграций»](data-model.md#ведение-seed-миграций). Практические шаги по загрузке тестовых данных описаны в [docs/testing-data.md](testing-data.md).

Базы данных.

Основные сервисы (Auth, CRM/Deals со встроенными задачами и уведомлениями, Documents) подключаются к единому PostgreSQL-кластеру. Изоляция достигается за счёт выделенных схем, отдельных ролей и политик row-level security там, где требуется. Кластер развёрнут в конфигурации primary–standby с резервным копированием в Backup-сервис. Платёжные данные хранятся в схеме `crm`: базовые реквизиты операций лежат в `crm.payments`, детализация поступлений — в `crm.payment_incomes`, удержаний — в `crm.payment_expenses`; из этих таблиц формируются события `deal.payment.updated`, `deal.payment.income.*` и `deal.payment.expense.*`. Для реактивного Auth переменные окружения используют R2DBC URI, например `r2dbc:postgresql://auth:auth@localhost:5432/crm?schema=auth`; допускается эквивалент с `search_path=auth`. CRM/Deals читает `search_path` или `options=-csearch_path=...` из DSN и передаёт в `server_settings` asyncpg, поэтому схема применяется одинаково в API и Alembic.

Брокеры сообщений и кэши.

RabbitMQ развёрнут в продакшене в кластере из трёх нод с quorum queue и mirrored classic queue для критичных очередей. Управление осуществляется через Kubernetes-оператор, конфигурации декларативно описаны в GitOps-репозитории. Для обеспечения надёжности включены автоматическое промоутирование лидеров, лимиты на TTL и длину очередей, а также политика dead-letter для повторяемых ошибок. Требования по мониторингу включают контроль глубины очередей, времени обработки сообщений и состояния потребителей; метрики собираются в Prometheus, алерты поддерживаются в Grafana, резервные копии конфигураций и политик выгружаются в Backup-сервис. Доменные события (`deal.created`, `deal.updated`, `deal.payment.updated`, `deal.payment.income.*`, `deal.payment.expense.*`, `task.requested` и т.д.) публикуются CRM/Deals в exchange `crm.events`, а внутренние модули задач и уведомлений подписываются на соответствующие routing key и транслируют их в SSE/Telegram.

Redis — общий высокодоступный кластер (Sentinel с не менее чем тремя узлами) для служебных ключей Gateway (heartbeat SSE), очередей Celery в CRM/Deals, задач BullMQ в Documents, а также FSM и rate limiting в Telegram Bot. Для отказоустойчивости настроены автоматическое переключение мастера с подтверждением репликации, лимиты на нагрузку по базам Redis и отдельные namespace/DB для сервисов. Мониторинг покрывает задержки, использование памяти и размер очередей Celery/BullMQ, а также состояние воркеров; показатели агрегируются в Prometheus с алертами в Grafana и логированием критичных событий в Loki. Резервирование выполняется через периодические снапшоты RDB/AOF, выгружаемые в Backup-сервис вместе с метаданными Sentinel.

Service discovery.

Consul развёрнут в Kubernetes через официальный Helm-чарт в конфигурации из трёх серверов и набора клиентских агентов на каждом
узле.

* Развёртывание: серверные узлы размещаются в отдельном StatefulSet с persistent volume и автоматическими health-checks;
  клиентские агенты доставляются DaemonSet-ом с sidecar-проверками для сервисов Gateway и внутренних API.
* Мониторинг: Consul-агенты экспортируют метрики в Prometheus, дашборды и алерты поддерживаются в Grafana, а технические события
  и аудит ACL транслируются в Loki.
* Резервирование и восстановление: снапшоты состояния ключ-значение и сервисных конфигураций отправляются в Backup-сервис,
  дополнительно по расписанию выгружаются ACL и intentions. Для DR-сценариев описаны инструкции восстановления кворума из
  последнего консистентного снапшота.

Контейнеризация и оркестрация.

Для локальной разработки сервисы поднимаются через Docker Compose.

В тестовой среде используется k3s, в продакшене — управляемый Kubernetes-кластер с Helm-чартами и сетевой политикой через Istio.

Логирование и мониторинг.

Логи собираются через Promtail в Loki.

Метрики собирает Prometheus, визуализация и алерты настраиваются в Grafana.

Трейсинг ключевых операций реализован через OpenTelemetry и Tempo.

CI/CD и GitOps.

Ранее конвейеры GitHub Actions выполняли lint/unit/contract-тесты для Gateway (Node.js/pnpm), проверку Gradle/Poetry-сервисов и сборку OCI-образов с публикацией в GHCR. Сейчас GitHub Actions удалён из репозитория, поэтому автоматических пайплайнов нет: повторяйте шаги вручную по инструкциям сервисов либо подключайте собственный CI в отдельной ветке/форке.

Переменная `CI_REGISTRY_IMAGE` по-прежнему используется как базовый префикс образов (`${CI_REGISTRY_IMAGE}-gateway` и т.д.) для локальных сборок и альтернативных CI.

Argo CD синхронизирует Kubernetes-кластер с каталогом `infra/k8s`: базовые манифесты (`namespace`, `Deployment`, `Service`, `ConfigMap`, `Secret`) и overlay (`overlays/dev|stage|prod`) описывают параметры Gateway и зависимостей (Redis), а Application-манифесты (`infra/k8s/argocd/gateway-apps.yaml`) подключают окружения с автоматической синхронизацией.

Интеграции
Серверное файловое хранилище

Documents-сервис больше не использует Drive API: файлы и папки размещаются в серверном каталоге, смонтированном в контейнер/виртуальную машину (локальный путь, подключённый том или S3-совместимый бакет, примонтированный через s3fs/rgw). Корневой каталог задаётся переменной `DOCUMENTS_STORAGE_ROOT`, шаблоны вложенных структур остаются прежними и строятся относительно этого пути. Клиентские библиотеки Google Drive и сопутствующие зависимости (`googleapis`, `google-auth-library`) исключены из поставки, чтобы сборка опиралась только на локальное файловое хранилище.

Правами доступа управляет сам сервис: при создании папок он применяет POSIX-права и ACL (команды `chmod`, `chown`, `setfacl`). Для корректной работы требуются утилиты `acl` и `attr` на хосте, а также системный пользователь, под которым запускается сервис, с правами на запись в каталог. Конфигурация пользователей/групп описана в [`backend/documents/README.md`](../backend/documents/README.md#права-доступа-и-пользователи).

Подписанные URL формируются для локального HTTP/объектного прокси (например, MinIO с включённым веб-сервером, nginx-webdav или встроенный файловый загрузчик). Значения `DOCUMENTS_UPLOAD_URL_BASE` и `DOCUMENTS_UPLOAD_URL_TTL` в [`env.example`](../env.example) отражают текущие требования.

Дополнительные зависимости сервиса Documents:

* системные утилиты `setfacl`/`getfacl` для управления ACL;
* `rsync` или `restic` (в зависимости от окружения) для бэкапов каталога документов;
* доступ к резервному хранилищу (SSH/S3) для выгрузки архивов;
* при сценариях S3 — установленный `s3fs`/`goofys` и unit `systemd` для автоматического монтирования (пример в README сервиса).

Метаданные файлов (относительный путь, ссылка на загрузку, владельцы-сущности, хэш, автор, временные метки) сохраняются в PostgreSQL Documents-сервиса. Для генерации публичной ссылки используйте `DOCUMENTS_FOLDERS_WEB_BASE_URL`, указывающую reverse-proxy или файловый браузер поверх каталога.

Telegram-бот

Бот принимает обновления через HTTPS-webhook, терминируемый в Gateway; используется обратный прокси с автоматическим обновлением TLS-сертификатов (Let’s Encrypt). Для локального тестирования webhook и Bot API подключайте mock-сервер по инструкции из [docs/local-setup.md#интеграции](local-setup.md#интеграции).

Команды и уведомления помещаются в очередь RabbitMQ, откуда их читают модуль уведомлений CRM и Telegram Bot. Ответы пользователям отправляются асинхронно.

Привязка Telegram-пользователя к учётной записи CRM запланирована и будет выполняться через REST-вызовы шлюза `/api/auth/telegram/*` после релиза соответствующих эндпоинтов.

Политика хранения импортируемых файлов

Файлы загружаются через Documents-сервис в каталог `DOCUMENTS_STORAGE_ROOT` (или примонтированный том). В PostgreSQL сохраняется только метаинформация (относительный путь, ссылки на загрузку/просмотр, привязки к сущностям); бинарные данные внутри базы не хранятся.

Служебные файлы, прошедшие импорт, помечаются TTL в 90 дней; по истечении срока Documents-сервис инициирует проверку продления и при отсутствии подтверждения переносит их в архив/удаляет с сервера (задача `documents.cleanup`).

Метаданные и события импорта фиксируются в журналах CRM и Reports и сохраняются бессрочно для восстановления истории действий.

Стек по сервисам

Gateway / BFF

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS с модулем @nestjs/axios для REST-проксирования

БД и очереди: ioredis (Redis); прямой работы с PostgreSQL нет

API: REST (JSON) и SSE-каналы (прокси потоков CRM `deals` и `notifications`), внутренние вызовы — REST и управление стримами

Зависимости и компоненты:

Redis для heartbeat и служебных ключей SSE

Service Discovery через Consul

SSE реализованы через декоратор `@Sse()` из `@nestjs/common` и `eventsource-parser`/RxJS для ретрансляции потоков

> ℹ️ Отдельный веб-клиент (Next.js) исключён из поставки: Gateway обслуживает внешние интеграции, внутренних операторов и Telegram-бота через REST/SSE API.

Тестирование и деплой:

Контрактные тесты и smoke-тесты API/интеграционных сценариев

Blue/green деплой без простоев

Auth

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Security)

БД и очереди: R2DBC (PostgreSQL), lettuce (Redis)

API: REST (`POST /api/auth/register`, `POST /api/auth/token`, `POST /api/auth/refresh`, `GET /api/auth/me`, `GET/POST /api/roles`, `GET/POST/DELETE /api/users/*`).

Зависимости:

PostgreSQL-схема auth (таблицы `users`, `roles`, `user_roles`).

Redis для хранения refresh-токенов (`auth:refresh:*`).

Текущая модель регистрации принимает email и пароль через DTO [`RegisterRequest`](../backend/auth/src/main/kotlin/com/crm/auth/api/dto/RegisterRequest.kt) и создаёт пользователя с базовой ролью `ROLE_USER`; выдача пары access/refresh токенов реализована в [`TokenService`](../backend/auth/src/main/kotlin/com/crm/auth/service/TokenService.kt) и использует Redis для хранения refresh-токенов с TTL. Управление ролями доступно администраторам через REST-эндпоинты [`/api/users`](../backend/auth/src/main/kotlin/com/crm/auth/api/UserController.kt) и [`/api/roles`](../backend/auth/src/main/kotlin/com/crm/auth/api/RoleController.kt), которые оперируют DTO `AssignRoleRequest`, `RoleRequest` и `RoleResponse`.

Ключевые переменные окружения (см. [`env.example`](../env.example)):

* `AUTH_DATABASE_URL`, `AUTH_REDIS_URL` — подключения к PostgreSQL (R2DBC) и Redis.
* `AUTH_JWT_SECRET`, `AUTH_JWT_ISSUER`, `AUTH_JWT_AUDIENCE` — параметры подписи и метаданные JWT.
* `AUTH_ACCESS_TOKEN_TTL`, `AUTH_REFRESH_TOKEN_TTL` — длительность токенов в формате ISO-8601 `Duration`.

Тестирование и деплой:

JUnit5 + Testcontainers

Миграции Liquibase, ротация JWT-ключей

CRM / Deals

Язык: Python 3.11

Фреймворк: FastAPI

БД и очереди: SQLAlchemy 2.0 + Alembic (PostgreSQL), Celery + Redis, RabbitMQ (aio-pika)

API: REST + SSE (через `sse-starlette` для стриминга событий сделок)

Зависимости:

PostgreSQL-схема crm

Redis (очередь Celery)

Использует общий кластер Redis, описанный в разделе «Брокеры сообщений и кэши», для Celery и очередей напоминаний задач.

RabbitMQ выступает шиной доменных событий: сервис публикует изменения в exchange `crm.events` (`deal.created`, `deal.updated`, `deal.payment.updated`, `deal.payment.income.*`, `deal.payment.expense.*`, документы; SLA-события добавятся после Этапа 1.1). Встроенные модули задач и уведомлений подписываются на соответствующие routing key (см. [архитектуру, раздел 2.2](architecture.md#22-асинхронная-шина-rabbitmq)) и ретранслируют события в SSE/Telegram. Подключение реализовано поверх `aio-pika` с подтверждением доставки (publisher confirms) и ручным ack со стороны консьюмера. Отдельных подписок на `payments.events` больше нет — платежи ведутся внутри CRM, а новые события по доходам и расходам позволяют отчётам получать агрегаты без опроса основного API.

Интеграция с Documents-сервисом: загрузки и скачивания проходят через его API, который создаёт каталоги и файлы в серверном хранилище (`DOCUMENTS_STORAGE_ROOT`). В PostgreSQL CRM хранит только метаданные (относительный путь, ссылки, связь с сущностями).

Тестирование и деплой:

Pytest + async-интеграции

Alembic миграции, прогрев кеша

Платёжный модуль
-----------------

Платёжная логика теперь встроена в сервис CRM/Deals: отдельный Spring Boot-сервис не используется. Все операции записываются в таблицы `crm.payments`, `crm.payment_incomes` и `crm.payment_expenses` внутри общей схемы `crm`. CRUD по платежам доступен через REST API CRM, а события `deal.payment.updated`, `deal.payment.income.*` и `deal.payment.expense.*` публикуются напрямую из CRM в exchange `crm.events`.

Documents

Язык: TypeScript (Node.js 20 LTS)

Фреймворк: NestJS 10 (`@nestjs/config`, `@nestjs/typeorm`, `@nestjs/bullmq`), файловые операции реализованы через стандартный модуль `fs/promises` и утилиты для управления правами.

БД и очереди: TypeORM (PostgreSQL, схема `documents`), BullMQ (Redis), очередь `documents_tasks` с заданиями `documents.upload` и `documents.sync`.

API: REST (CRUD метаданных, health-check), отдельный воркер BullMQ.

Зависимости:

PostgreSQL-схема `documents` (расширение `pgcrypto` для `gen_random_uuid()` теперь создаётся самими миграциями сервиса, вручную его включать не требуется).

Redis (`DOCUMENTS_REDIS_URL`, `DOCUMENTS_REDIS_PREFIX`).

Серверный каталог (`DOCUMENTS_STORAGE_ROOT`) с доступом пользователя сервиса, установленными утилитами `setfacl/getfacl`, `rsync`/`restic` для бэкапов и (опционально) systemd-unit для монтирования тома/S3 (см. [`backend/documents/README.md`](../backend/documents/README.md)).

Переменные `DOCUMENTS_QUEUE_NAME`, `DOCUMENTS_PERMISSIONS_SYNC_QUEUE_NAME` и `DOCUMENTS_RUN_MIGRATIONS` управляют именами очередей BullMQ и автозапуском миграций TypeORM; названия очередей не должны содержать двоеточий.

Тестирование и деплой:

E2E-тесты NestJS, smoke-запуск воркера; миграции применяются командой `pnpm typeorm migration:run -d typeorm.config.ts`.


Встроенные задачи и уведомления CRM

CRM/Deals включает модуль задач и систему уведомлений без отдельного сервиса. Постоянные данные задач и напоминаний находятся в схеме `tasks`, которую мигрируют Alembic-скрипты CRM; отложенные обработки выполняются через Celery (Redis) и фоновые корутины. RabbitMQ exchange `tasks.events` используется для публикации CloudEvents задач (`task.created`, `task.status.changed`, `task.reminder`), а модуль уведомлений транслирует события `notification.*` в SSE-канал `notifications` и очереди Telegram-бота. Конфигурация управляется переменными `CRM_TASKS_*`, `CRM_EVENTS_EXCHANGE` и `CRM_CELERY_*`, которые описаны в [`env.example`](../env.example). Для тестирования используются pytest и интеграционные проверки SSE; при bootstrap smoke-тесты покрывают публикацию задач и получение уведомлений через Gateway.

Telegram Bot

Язык: Python 3.11

Фреймворк: aiogram 3

БД и очереди: asyncpg (PostgreSQL), aio-pika (RabbitMQ)

API: Вебхуки Telegram + внутренний REST Callback

Зависимости:

PostgreSQL-схема bot (readonly к auth и crm)

Очередь notifications.telegram

Redis для FSM и rate limiting

Работает поверх общего Redis-кластера с Sentinel, покрытого мониторингом и бэкапами, описанными в разделе «Брокеры сообщений и кэши».

Тестирование и деплой:

Pytest-asyncio + моки Telegram API

End-to-end сценарии в staging, blue/green деплой

Reports

Язык: Python 3.11

Фреймворк: FastAPI + SQLAlchemy 2.0 (async)

БД и источники: PostgreSQL (материализованные представления в схеме `reports`, чтение агрегатов `crm`), asyncpg

API: REST (`/api/v1/aggregates/**`), health-check `/health`

Зависимости:

PostgreSQL-схема reports и доступ на чтение к таблицам/представлениям CRM

Материализованное представление `deal_pipeline_summary`, поддерживаемое SQL-миграциями и CLI `reports-refresh-views`

Gateway подключит публичные маршруты отчётности (в планах интеграции)

Тестирование и деплой:

Pytest + HTTPX (юнит и contract-тесты REST API)

Poetry-скрипты (`reports-api`, `reports-refresh-views`); миграции — SQL-файлы в `backend/reports/migrations`

<a id="backup"></a>Backup

Язык: Python 3.11

Фреймворк: FastAPI + APScheduler

БД и очереди: psycopg (PostgreSQL), boto3 (S3-совместимое хранилище), aio-pika (RabbitMQ для уведомлений)

API: REST для управления бэкапами, health-check реализован через REST endpoint `/health` и метрики Prometheus

Механизмы резервного копирования:

* базы PostgreSQL выгружаются через `pg_dump` (инкрементальные схемные бэкапы) и `pg_basebackup` для полного восстановления; артефакты сжимаются и отправляются в версионируемое S3-хранилище;
* Consul снапшоты получаются через `consul snapshot save`, RabbitMQ конфигурации и политики выгружаются с помощью `rabbitmqadmin export` и хранятся вместе с базовыми бэкапами;
* Redis снапшоты RDB/AOF, включая базы Celery и BullMQ, выполняются с контролем успешной репликации Sentinel и загружаются в то же S3-хранилище, что и другие инфраструктурные бэкапы;
* журналы выполнения задач и контрольные суммы артефактов пишутся в таблицу backup.jobs, что позволяет перезапускать операции и отслеживать SLA.

Оркестрация и уведомления:

* APScheduler формирует расписания ежедневных, еженедельных и внеочередных бэкапов;
* статусы задач публикуются в очередь backup.notifications (RabbitMQ), откуда модуль уведомлений CRM отправляет предупреждения дежурной команде;
* S3-хранилище монтируется через CSI-драйвер, секреты доступа подтягиваются из Vault;
* Kubernetes выполняет liveness/readiness-пробы по `GET /healthz`, а результаты проб фиксируются в метриках наблюдаемости.

Тестирование и деплой:

* pytest + moto для имитации S3, Testcontainers для PostgreSQL и RabbitMQ, отдельные smoke-тесты вызывают `GET /healthz` после выката;
* деплой как Kubernetes CronJob + вспомогательный Deployment для REST-API управления, использование init-контейнеров для проверки доступности хранилищ и автоматическая проверка `/healthz` перед переводом трафика;
* политики ретенции управляются через Helm values и проверяются линтером kube-score в CI.

Дальнейшее развитие

Список технологий будет расширяться по мере детализации сервисов и инфраструктуры (СУБД, брокеры, деплой, интеграции).
Для Auth в дорожной карте остаётся привязка Telegram-аккаунтов и подтверждение операций через бота; требования уточняются в [плане поставки](delivery-plan.md) и пока не поддерживаются текущим REST API.【F:docs/delivery-plan.md†L1-L35】
Все изменения документируются в docs/ и проходят архитектурное ревью.
