Назначение документа

Документ фиксирует базовые архитектурные принципы и технологические решения CRM-системы.
Он служит отправной точкой для команд разработки, эксплуатации и бизнес-заказчиков, помогая синхронизировать ожидания относительно сервисов и инфраструктуры.
Материал дополняет обзорные документы (README.md, architecture.md, security-and-access.md) и задаёт общие правила для детальных спецификаций отдельных сервисов. Состав первой поставки и приоритизация следующих этапов фиксируются в [плане поставки](delivery-plan.md), который следует использовать в качестве точки входа для планирования релизов. За обзором взаимодействий и потоков данных обращайтесь к разделу [«Взаимодействия и потоки данных»](architecture.md#2-взаимодействия-и-потоки-данных).

Общие архитектурные принципы

Микросервисный подход. Функциональные домены выделяются в самостоятельные сервисы с чёткими API-контрактами, управляемыми через Gateway или BFF (подробнее — [раздел 2.4 архитектуры](architecture.md#24-роль-gatewaybff)).

Единая база данных. Сервисы делят общий PostgreSQL-кластер, изолируясь на уровне схем, ролей и политик доступа; при существенном росте нагрузки возможен поэтапный переход к выделенным кластерам для отдельных доменов (см. [описание кластера](architecture.md#23-единый-postgresql-кластер)).

Асинхронные коммуникации. Для обмена событиями и фоновых задач применяются очереди сообщений (RabbitMQ) и публикация доменных событий; композицию потоков см. в [разделе 2.2 архитектуры](architecture.md#22-асинхронная-шина-rabbitmq).

Кеширование. Для ускорения чтения справочных и агрегированных данных используется Redis или аналогичный in-memory кеш с TTL и контролем согласованности.

Наблюдаемость и устойчивость. Все сервисы обязаны вести структурированные логи, метрики и трассировки (Prometheus, Grafana, OpenTelemetry), обеспечивать алерты и обработку ошибок с повторными попытками.

Управление конфигурацией и секретами. Конфигурации хранятся централизованно (Vault/SSM/Secrets Manager), секреты не попадают в репозиторий, развёртывание автоматизируется через CI/CD.

Безопасность по умолчанию. Реализованы единые механизмы аутентификации/авторизации, шифрование трафика, контроль доступа и регулярные проверки на соответствие требованиям безопасности.

Инфраструктура

Расширенный пример переменных окружения для локальной разработки и CI/CD собран в файле [`env.example`](../env.example). Он отражает секции «Базы данных», «Брокеры сообщений и кэши» и «Service discovery», а также интеграции (серверное файловое хранилище, Telegram), описанные ниже.

Политика ведения seed-миграций и владение справочниками зафиксированы в разделе [«Ведение seed-миграций»](data-model.md#ведение-seed-миграций). Практические шаги по загрузке тестовых данных описаны в [docs/testing-data.md](testing-data.md).

Базы данных.

Основные сервисы (Auth, CRM/Deals, Tasks, Notifications) подключаются к единому PostgreSQL-кластеру. Изоляция достигается за счёт выделенных схем, отдельных ролей и политик row-level security там, где требуется. Кластер развёрнут в конфигурации primary–standby с резервным копированием в Backup-сервис. Платёжные данные хранятся в схеме `crm`: базовые реквизиты операций лежат в `crm.payments`, детализация поступлений — в `crm.payment_incomes`, удержаний — в `crm.payment_expenses`; из этих таблиц формируются события `deal.payment.updated`, `deal.payment.income.*` и `deal.payment.expense.*`. Для реактивных приложений (Auth, Audit) переменные окружения используют R2DBC URI, например `r2dbc:postgresql://auth:auth@localhost:5432/crm?schema=auth`; допускается эквивалент с `search_path=auth`. CRM/Deals читает `search_path` или `options=-csearch_path=...` из DSN и передаёт в `server_settings` asyncpg, поэтому схема применяется одинаково в API и Alembic.

Audit использует ту же инсталляцию PostgreSQL, но ведёт журналы в собственной схеме с включённым логированием длительных транзакций и повышенными SLA на хранение.

Брокеры сообщений и кэши.

RabbitMQ развёрнут в продакшене в кластере из трёх нод с quorum queue и mirrored classic queue для критичных очередей. Управление осуществляется через Kubernetes-оператор, конфигурации декларативно описаны в GitOps-репозитории. Для обеспечения надёжности включены автоматическое промоутирование лидеров, лимиты на TTL и длину очередей, а также политика dead-letter для повторяемых ошибок. Требования по мониторингу включают контроль глубины очередей, времени обработки сообщений и состояния потребителей; метрики собираются в Prometheus, алерты поддерживаются в Grafana, резервные копии конфигураций и политик выгружаются в Backup-сервис. Доменные события (`deal.created`, `deal.updated`, `deal.payment.updated`, `deal.payment.income.*`, `deal.payment.expense.*`, `task.requested` и т.д.) публикуются CRM/Deals в exchange `crm.events`, а Notifications и Tasks подписываются на соответствующие routing key.

Redis — общий высокодоступный кластер (Sentinel с не менее чем тремя узлами) для кэширования сессий Gateway, очередей Celery в CRM/Deals, задач BullMQ в Documents, а также FSM и rate limiting в Telegram Bot и Notifications. Для отказоустойчивости настроены автоматическое переключение мастера с подтверждением репликации, лимиты на нагрузку по базам Redis и отдельные namespace/DB для сервисов. Мониторинг покрывает задержки, использование памяти и размер очередей Celery/BullMQ, а также состояние воркеров; показатели агрегируются в Prometheus с алертами в Grafana и логированием критичных событий в Loki. Резервирование выполняется через периодические снапшоты RDB/AOF, выгружаемые в Backup-сервис вместе с метаданными Sentinel.

Service discovery.

Consul развёрнут в Kubernetes через официальный Helm-чарт в конфигурации из трёх серверов и набора клиентских агентов на каждом
узле.

* Развёртывание: серверные узлы размещаются в отдельном StatefulSet с persistent volume и автоматическими health-checks;
  клиентские агенты доставляются DaemonSet-ом с sidecar-проверками для сервисов Gateway и внутренних API.
* Мониторинг: Consul-агенты экспортируют метрики в Prometheus, дашборды и алерты поддерживаются в Grafana, а технические события
  и аудит ACL транслируются в Loki.
* Резервирование и восстановление: снапшоты состояния ключ-значение и сервисных конфигураций отправляются в Backup-сервис,
  дополнительно по расписанию выгружаются ACL и intentions. Для DR-сценариев описаны инструкции восстановления кворума из
  последнего консистентного снапшота.

Контейнеризация и оркестрация.

Для локальной разработки сервисы поднимаются через Docker Compose.

В тестовой среде используется k3s, в продакшене — управляемый Kubernetes-кластер с Helm-чартами и сетевой политикой через Istio.

Логирование и мониторинг.

Логи собираются через Promtail в Loki.

Метрики собирает Prometheus, визуализация и алерты настраиваются в Grafana.

Трейсинг ключевых операций реализован через OpenTelemetry и Tempo.

CI/CD и GitOps.

Ранее конвейеры GitHub Actions выполняли lint/unit/contract-тесты для Gateway (Node.js/pnpm), проверку Gradle/Poetry-сервисов и сборку OCI-образов с публикацией в GHCR. Сейчас GitHub Actions удалён из репозитория, поэтому автоматических пайплайнов нет: повторяйте шаги вручную по инструкциям сервисов либо подключайте собственный CI в отдельной ветке/форке.

Переменная `CI_REGISTRY_IMAGE` по-прежнему используется как базовый префикс образов (`${CI_REGISTRY_IMAGE}-gateway` и т.д.) для локальных сборок и альтернативных CI.

Argo CD синхронизирует Kubernetes-кластер с каталогом `infra/k8s`: базовые манифесты (`namespace`, `Deployment`, `Service`, `ConfigMap`, `Secret`) и overlay (`overlays/dev|stage|prod`) описывают параметры Gateway и зависимостей (Redis), а Application-манифесты (`infra/k8s/argocd/gateway-apps.yaml`) подключают окружения с автоматической синхронизацией.

Интеграции
Серверное файловое хранилище

Documents-сервис больше не использует Drive API: файлы и папки размещаются в серверном каталоге, смонтированном в контейнер/виртуальную машину (локальный путь, подключённый том или S3-совместимый бакет, примонтированный через s3fs/rgw). Корневой каталог задаётся переменной `DOCUMENTS_STORAGE_ROOT`, шаблоны вложенных структур остаются прежними и строятся относительно этого пути. Клиентские библиотеки Google Drive и сопутствующие зависимости (`googleapis`, `google-auth-library`) исключены из поставки, чтобы сборка опиралась только на локальное файловое хранилище.

Правами доступа управляет сам сервис: при создании папок он применяет POSIX-права и ACL (команды `chmod`, `chown`, `setfacl`). Для корректной работы требуются утилиты `acl` и `attr` на хосте, а также системный пользователь, под которым запускается сервис, с правами на запись в каталог. Конфигурация пользователей/групп описана в [`backend/documents/README.md`](../backend/documents/README.md#права-доступа-и-пользователи).

Подписанные URL формируются для локального HTTP/объектного прокси (например, MinIO с включённым веб-сервером, nginx-webdav или встроенный файловый загрузчик). Значения `DOCUMENTS_UPLOAD_URL_BASE` и `DOCUMENTS_UPLOAD_URL_TTL` в [`env.example`](../env.example) отражают текущие требования.

Дополнительные зависимости сервиса Documents:

* системные утилиты `setfacl`/`getfacl` для управления ACL;
* `rsync` или `restic` (в зависимости от окружения) для бэкапов каталога документов;
* доступ к резервному хранилищу (SSH/S3) для выгрузки архивов;
* при сценариях S3 — установленный `s3fs`/`goofys` и unit `systemd` для автоматического монтирования (пример в README сервиса).

Метаданные файлов (относительный путь, ссылка на загрузку, владельцы-сущности, хэш, автор, временные метки) сохраняются в PostgreSQL Documents-сервиса. Для генерации публичной ссылки используйте `DOCUMENTS_FOLDERS_WEB_BASE_URL`, указывающую reverse-proxy или файловый браузер поверх каталога.

Telegram-бот

Бот принимает обновления через HTTPS-webhook, терминируемый в Gateway; используется обратный прокси с автоматическим обновлением TLS-сертификатов (Let’s Encrypt). Для локального тестирования webhook и Bot API подключайте mock-сервер по инструкции из [docs/local-setup.md#интеграции](local-setup.md#интеграции).

Команды и уведомления помещаются в очередь RabbitMQ, откуда их читают Notifications и CRM/Deals. Ответы пользователям отправляются асинхронно.

Привязка Telegram-пользователя к учётной записи CRM хранится в Auth-сервисе и выполняется через REST-вызовы шлюза `/api/auth/telegram/*`.

Политика хранения импортируемых файлов

Файлы загружаются через Documents-сервис в каталог `DOCUMENTS_STORAGE_ROOT` (или примонтированный том). В PostgreSQL сохраняется только метаинформация (относительный путь, ссылки на загрузку/просмотр, привязки к сущностям); бинарные данные внутри базы не хранятся.

Служебные файлы, прошедшие импорт, помечаются TTL в 90 дней; по истечении срока Documents-сервис инициирует проверку продления и при отсутствии подтверждения переносит их в архив/удаляет с сервера (задача `documents.cleanup`).

Метаданные и события импорта фиксируются в Audit и сохраняются бессрочно для восстановления истории действий.

Стек по сервисам

Фронтенд

Ядро: React 19 + TypeScript, собранные поверх Next.js 15 в режиме App Router для гибридного SSR/SSG и оптимизаций по производительности.

Сборка и поставка: встроенный bundler Next.js (SWC) и Turbopack в режиме разработки; артефакт собирается в Docker-образ, который публикуется вручную (или через сторонний CI) в GitHub Container Registry и доставляется в Kubernetes через Argo CD (отдельный deployment `frontend`).

Управление состоянием: React Query для работы с асинхронными данными Gateway/BFF, Zustand для локального UI-состояния и кэширования пользовательских настроек, Context API для тем и параметров локализации.

Взаимодействие с Gateway/BFF: REST-запросы через обёртку вокруг `fetch` с автоматическим проставлением токена сессии (httpOnly cookie); в первой поставке фронтенд открывает два SSE-канала — сделки (`deals`) и внутренние уведомления (`notifications`). Все вызовы проходят через версионированный префикс `/api/v1`, домен и адреса стриминговых каналов задаются переменными окружения (`NEXT_PUBLIC_CRM_SSE_URL`, `NEXT_PUBLIC_NOTIFICATIONS_SSE_URL`). Значения по умолчанию в [`env.example`](../env.example) указывают на документированные маршруты Gateway в Docker-сети — `http://gateway:8080/api/v1/streams/deals` и `http://gateway:8080/api/v1/streams/notifications`. При запуске фронтенда вне Docker переопределите их на `http://localhost:${GATEWAY_SERVICE_PORT}/api/v1/streams/...`, чтобы браузер обращался к шлюзу на хостовой машине. Обновления оплат приходят в составе событий `deal.updated` канала `deals`, после чего фронтенд инвалидациирует кеш React Query и показывает тост «Оплата подтверждена». Для окружения разработки (`dev`) фронтенд использует `http://localhost`, тогда как значения для stage/prod поступают из Kubernetes overlay (`infra/k8s/overlays`) и резолвятся в актуальные доменные имена. Внутренний upstream для CRM продолжает обозначаться как `crm`, однако публичный маршрут и фронтенд-конфигурация используют имя `deals` для единообразия. Отдельный поток задач появится после расширения сценариев напоминаний; в текущем релизе соединение с Tasks не открывается. Значения для окружений stage/prod подставляются через Kubernetes overlay, поэтому при переключении окружения обновлять `.env` вручную не требуется.

Тестирование: unit и компонентные тесты на Vitest + React Testing Library, визуальные снапшоты Storybook Chromatic, end-to-end сценарии в Playwright. Для всех команд зафиксирован менеджер пакетов `pnpm` версии 9 через Corepack. Smoke-тесты фронтенда запускаются после деплоя вместе с контрактными тестами Gateway/BFF.

Требования к окружению:

* `NEXT_PUBLIC_API_BASE_URL` — публичный URL Gateway/BFF, используемый при серверном рендеринге и на клиенте (локально по умолчанию `http://localhost:${GATEWAY_SERVICE_PORT}/api/v1`, то есть `http://localhost:8080/api/v1` при стандартном значении порта).
* `NEXT_PUBLIC_TELEMETRY_DSN` — DSN для фронтенд-логирования/трейсинга (Sentry или аналог), передаётся в runtime.
* `NEXT_PUBLIC_FEATURE_FLAGS` — перечисление включённых feature-флагов (через запятую), синхронизировано с LaunchDarkly/ConfigCat.
* `FRONTEND_PROXY_TIMEOUT` — таймаут (мс) для проксирования на уровне Next.js middleware и браузерных запросов встроенного API-клиента.
* `FRONTEND_SERVER_TIMEOUT_MS` — серверный таймаут (мс) для SSR и серверных экшенов Next.js; по умолчанию 7500 мс, чтобы ускорить откат к мок-данным или показать ошибку.

Связь с инфраструктурой: переменные окружения подставляются через Secrets Manager → Kubernetes secrets; окружения `dev/stage/prod` получают собственные конфигурации API и ключей телеметрии. Проверку соответствия схем API и e2e-сценариев выполняйте вручную или в подключённом альтернативном CI перед синхронизацией Argo CD. Для локального запуска значения считываются из файла `.env.local`, основанного на [`env.example`](../env.example) в корне репозитория.

Gateway / BFF

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS с модулем @nestjs/axios для агрегации

БД и очереди: ioredis (Redis); прямой работы с PostgreSQL нет

API: REST (JSON) и SSE-каналы (прокси потоков CRM/Deals и Notifications; отдельный канал Tasks отложен до следующих релизов), внутренние вызовы — REST и управление стримами

Зависимости и компоненты:

Redis для сессий и кеша

Service Discovery через Consul

SSE реализованы через декоратор `@Sse()` из `@nestjs/common` и `eventsource-parser`/RxJS для ретрансляции потоков

Тестирование и деплой:

Контрактные тесты и smoke-тесты UI

Blue/green деплой без простоев

Auth

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Security)

БД и очереди: R2DBC (PostgreSQL), lettuce (Redis)

API: REST + OAuth2/OpenID Connect

Зависимости:

PostgreSQL-схема auth

Redis для токенов и кодов

Notifications/Telegram Bot для выдачи одноразовых подтверждений входа, активации новых аккаунтов и быстрой валидации операций

CRM/Deals инициирует приглашение пользователя, а Telegram Bot доставляет одноразовый токен активации; подтверждение учётной записи происходит в Auth по предъявлению токена

Внутренний сервис администрирования для генерации стартовых токенов и восстановления доступа через CRM/Telegram с протоколированием операций в Audit

Ключевые переменные окружения (см. [`env.example`](../env.example)):

* `AUTH_DATABASE_URL`, `AUTH_REDIS_URL` — подключения к PostgreSQL (R2DBC) и Redis.
* `AUTH_JWT_SECRET`, `AUTH_JWT_ISSUER`, `AUTH_JWT_AUDIENCE` — параметры подписи и метаданные JWT.
* `AUTH_ACCESS_TOKEN_TTL`, `AUTH_REFRESH_TOKEN_TTL` — длительность токенов в формате ISO-8601 `Duration`.

Тестирование и деплой:

JUnit5 + Testcontainers

Миграции Liquibase, ротация JWT-ключей

CRM / Deals

Язык: Python 3.11

Фреймворк: FastAPI

БД и очереди: SQLAlchemy 2.0 + Alembic (PostgreSQL), Celery + Redis, RabbitMQ (aio-pika)

API: REST + SSE (через `sse-starlette` для стриминга событий сделок)

Зависимости:

PostgreSQL-схема crm

Redis (очередь Celery)

Использует общий кластер Redis, описанный в разделе «Брокеры сообщений и кэши», и разделяет мониторинг очередей Celery с Notifications.

RabbitMQ выступает шиной доменных событий: сервис публикует изменения в exchange `crm.events` (`deal.created`, `deal.updated`, `deal.payment.updated`, `deal.payment.income.*`, `deal.payment.expense.*`, документы; SLA-события добавятся после Этапа 1.1). Notifications и Tasks подписываются на соответствующие routing key (см. [архитектуру, раздел 2.2](architecture.md#22-асинхронная-шина-rabbitmq)). Подключение реализовано поверх `aio-pika` с подтверждением доставки (publisher confirms) и ручным ack со стороны консьюмера. Отдельных подписок на `payments.events` больше нет — платежи ведутся внутри CRM, а новые события по доходам и расходам позволяют отчётам получать агрегаты без опроса основного API.

Интеграция с Documents-сервисом: загрузки и скачивания проходят через его API, который создаёт каталоги и файлы в серверном хранилище (`DOCUMENTS_STORAGE_ROOT`). В PostgreSQL CRM хранит только метаданные (относительный путь, ссылки, связь с сущностями).

Тестирование и деплой:

Pytest + async-интеграции

Alembic миграции, прогрев кеша

Платёжный модуль
-----------------

Платёжная логика теперь встроена в сервис CRM/Deals: отдельный Spring Boot-сервис не используется. Все операции записываются в таблицы `crm.payments`, `crm.payment_incomes` и `crm.payment_expenses` внутри общей схемы `crm`. CRUD по платежам доступен через REST API CRM, а события `deal.payment.updated`, `deal.payment.income.*` и `deal.payment.expense.*` публикуются напрямую из CRM в exchange `crm.events`.

Documents

Язык: TypeScript (Node.js 20 LTS)

Фреймворк: NestJS 10 (`@nestjs/config`, `@nestjs/typeorm`, `@nestjs/bullmq`), файловые операции реализованы через стандартный модуль `fs/promises` и утилиты для управления правами.

БД и очереди: TypeORM (PostgreSQL, схема `documents`), BullMQ (Redis), очередь `documents:tasks` с заданиями `documents.upload` и `documents.sync`.

API: REST (CRUD метаданных, health-check), отдельный воркер BullMQ.

Зависимости:

PostgreSQL-схема `documents` (не забудьте включить `pgcrypto` для `gen_random_uuid()`).

Redis (`DOCUMENTS_REDIS_URL`, `DOCUMENTS_REDIS_PREFIX`).

Серверный каталог (`DOCUMENTS_STORAGE_ROOT`) с доступом пользователя сервиса, установленными утилитами `setfacl/getfacl`, `rsync`/`restic` для бэкапов и (опционально) systemd-unit для монтирования тома/S3 (см. [`backend/documents/README.md`](../backend/documents/README.md)).

Переменные `DOCUMENTS_QUEUE_NAME` и `DOCUMENTS_RUN_MIGRATIONS` управляют именем очереди BullMQ и автозапуском миграций TypeORM.

Тестирование и деплой:

E2E-тесты NestJS, smoke-запуск воркера; миграции применяются командой `pnpm typeorm migration:run -d typeorm.config.ts`.

Tasks и Notifications могут развёртываться в одной инфраструктурной связке (общий репозиторий, пайплайн, shared-модули NestJS),
но остаются отдельными сервисами с собственными схемами БД и очередями. Ниже приведены их стек и зависимости.

Tasks

Поставляемая версия реализует REST-API, хранение задач в PostgreSQL и отложенные напоминания через Redis. Воркеры активируют задачи по расписанию, публикуя события в RabbitMQ; SLA и повторяющиеся сценарии остаются в плане [Этапа 1.1](delivery-plan.md#2-приоритизация-последующих-этаов).

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS (ConfigModule, @nestjs/schedule, CQRS-модуль и @nestjs/microservices для RabbitMQ)

БД и очереди: TypeORM (PostgreSQL, схема `tasks`), RabbitMQ transport NestJS, Redis (ioredis) для отложенной очереди

API: REST (`/api/tasks`) и фоновые воркеры (`pnpm worker`, включают `TASKS_WORKER_ENABLED`)

Зависимости:

PostgreSQL-схема tasks

Topic-exchange RabbitMQ `tasks.events` для публикации CloudEvents задач (`task.created`, `task.status.changed`, `task.reminder`; `source = tasks.service`)

Redis (sorted set `TASKS_DELAYED_QUEUE_KEY`) для хранения таймеров

Notifications API и CRM используют RabbitMQ события задач для синхронизации (подписчики подключаются через общую шину)

Тестирование и деплой:

Jest + supertest (при появлении тестов в репозитории), smoke-check REST и воркера в bootstrap

TypeORM миграции, запуск сидов статусов через `pnpm seed:statuses`

Notifications

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS (@nestjs/platform-express, @nestjs/config, @golevelup/nestjs-rabbitmq)

БД и очереди: TypeORM (PostgreSQL, схема `notifications`), @golevelup/nestjs-rabbitmq (RabbitMQ), @liaoliaots/nestjs-redis (Redis)

API: REST (`POST /notifications/events` для ручных публикаций) + SSE (`GET /notifications/stream`), подписчик очереди `notifications.events`, рассылка в Telegram через Bot API с поддержкой mock-режима

Зависимости:

PostgreSQL-схема notifications (таблицы `notification_events` и `notification_templates` — последняя хранит тело шаблонов с составным ключом `key` + `channel` и служебными полями `locale`, `metadata`, `status`)

RabbitMQ exchange `notifications.events` + очередь `notifications.events` с настройкой durability и routing key `notifications.*`

Redis namespace `notifications:` для stateful-компонентов (повторные подключения SSE, throttling Telegram)

Gateway маршрутизирует внешние webhook-и Telegram; Tasks и модуль платежей CRM публикуют события, на которые подписывается сервис

Тестирование и деплой:

TypeORM миграции, smoke-тесты SSE-канала, потребительские проверки очередей RabbitMQ, Jest-спеки для REST (`/api/v1/templates`) и бизнес-логики шаблонов

Отдельные процессы для HTTP (`pnpm start:dev`) и воркеров (`pnpm start:workers`), canary-релизы с мониторингом доставки сообщений

Telegram Bot

Язык: Python 3.11

Фреймворк: aiogram 3

БД и очереди: asyncpg (PostgreSQL), aio-pika (RabbitMQ)

API: Вебхуки Telegram + внутренний REST Callback

Зависимости:

PostgreSQL-схема bot (readonly к auth и crm)

Очередь notifications.telegram

Redis для FSM и rate limiting

Работает поверх общего Redis-кластера с Sentinel, покрытого мониторингом и бэкапами, описанными в разделе «Брокеры сообщений и кэши».

Тестирование и деплой:

Pytest-asyncio + моки Telegram API

End-to-end сценарии в staging, blue/green деплой

<a id="audit"></a>Audit

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Actuator, Spring Cloud Stream)

БД и очереди: Spring Data R2DBC (PostgreSQL), Spring AMQP (RabbitMQ)

API: REST endpoints для внутренних подписчиков (`GET /api/audit/events` с пагинацией и фильтрами по типу/времени); события доставляются через Spring Cloud Stream

Интеграция с PostgreSQL:

* таблица `audit.audit_events` секционирована по диапазону `occurred_at`: хранится историческая партиция для прошлых данных, на ближайшие 12 месяцев заранее создаются помесячные секции, есть дефолтная партиция и функция `audit.ensure_audit_events_partition(date)` для оперативного добавления новых; контроль долгих транзакций и ретеншн реализуются на уровне политик;
* запись событий выполняется через Spring Data R2DBC-репозиторий с контролем идемпотентности по `event_id` либо сочетанию типа, времени и источника;
* ежедневная агрегация метрик действий пользователей формируется через Spring Batch и записывается в материализованные представления, которые потребляются Reports и внутренними дашбордами.

Интеграция с RabbitMQ:

* сервис подписывается на exchange audit.events и очередь audit.core с quorum queue; сообщения публикуют CRM/Deals, Auth и Notifications;
* включены ручные подтверждения доставки после успешной записи в БД, сохраняется `message_id` для идемпотентности и контроля повторных доставок, отдельная очередь audit.dlq отслеживается через Spring AMQP;
* при сбое основного хранилища события буферизуются в локальном write-ahead журнале на PersistentVolume до восстановления соединения с PostgreSQL.

Тестирование и деплой:

* Интеграционные тесты на JUnit5 + Testcontainers (PostgreSQL, RabbitMQ);
* Развёртывание через Kubernetes StatefulSet с подстроенными ресурсными квотами, rolling update с прогревом кэша справочников;
* Конфигурация очередей описывается в Helm-чарте и синхронизируется Argo CD; миграции схемы управляются через Liquibase в CI/CD.

Reports

Язык: Python 3.11

Фреймворк: FastAPI + SQLAlchemy 2.0 (async)

БД и источники: PostgreSQL (материализованные представления в схеме `reports`, чтение агрегатов `crm` и `audit`), asyncpg

API: REST (`/api/v1/aggregates/**`), health-check `/health`

Зависимости:

PostgreSQL-схема reports и доступ на чтение к таблицам/представлениям CRM и Audit

Материализованное представление `deal_pipeline_summary`, поддерживаемое SQL-миграциями и CLI `reports-refresh-views`

Gateway подключит публичные маршруты отчётности (в планах интеграции)

Тестирование и деплой:

Pytest + HTTPX (юнит и contract-тесты REST API)

Poetry-скрипты (`reports-api`, `reports-refresh-views`); миграции — SQL-файлы в `backend/reports/migrations`

<a id="backup"></a>Backup

Язык: Python 3.11

Фреймворк: FastAPI + APScheduler

БД и очереди: psycopg (PostgreSQL), boto3 (S3-совместимое хранилище), aio-pika (RabbitMQ для уведомлений)

API: REST для управления бэкапами, health-check реализован через REST endpoint `/health` и метрики Prometheus

Механизмы резервного копирования:

* базы PostgreSQL выгружаются через `pg_dump` (инкрементальные схемные бэкапы) и `pg_basebackup` для полного восстановления; артефакты сжимаются и отправляются в версионируемое S3-хранилище;
* Consul снапшоты получаются через `consul snapshot save`, RabbitMQ конфигурации и политики выгружаются с помощью `rabbitmqadmin export` и хранятся вместе с базовыми бэкапами;
* Redis снапшоты RDB/AOF, включая базы Celery и BullMQ, выполняются с контролем успешной репликации Sentinel и загружаются в то же S3-хранилище, что и другие инфраструктурные бэкапы;
* журналы выполнения задач и контрольные суммы артефактов пишутся в таблицу backup.jobs, что позволяет перезапускать операции и отслеживать SLA.

Оркестрация и уведомления:

* APScheduler формирует расписания ежедневных, еженедельных и внеочередных бэкапов;
* статусы задач публикуются в очередь backup.notifications (RabbitMQ), откуда Notifications отправляет предупреждения дежурной команде;
* S3-хранилище монтируется через CSI-драйвер, секреты доступа подтягиваются из Vault;
* Kubernetes выполняет liveness/readiness-пробы по `GET /healthz`, а результаты проб фиксируются в метриках наблюдаемости.

Тестирование и деплой:

* pytest + moto для имитации S3, Testcontainers для PostgreSQL и RabbitMQ, отдельные smoke-тесты вызывают `GET /healthz` после выката;
* деплой как Kubernetes CronJob + вспомогательный Deployment для REST-API управления, использование init-контейнеров для проверки доступности хранилищ и автоматическая проверка `/healthz` перед переводом трафика;
* политики ретенции управляются через Helm values и проверяются линтером kube-score в CI.

Дальнейшее развитие

Список технологий будет расширяться по мере детализации сервисов и инфраструктуры (СУБД, брокеры, деплой, интеграции).
Все изменения документируются в docs/ и проходят архитектурное ревью.