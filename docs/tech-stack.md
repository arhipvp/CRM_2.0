Назначение документа

Документ фиксирует базовые архитектурные принципы и технологические решения CRM-системы.
Он служит отправной точкой для команд разработки, эксплуатации и бизнес-заказчиков, помогая синхронизировать ожидания относительно сервисов и инфраструктуры.
Материал дополняет обзорные документы (README.md, architecture.md, security-and-access.md) и задаёт общие правила для детальных спецификаций отдельных сервисов. Состав первой поставки и приоритизация следующих этапов фиксируются в [плане поставки](delivery-plan.md), который следует использовать в качестве точки входа для планирования релизов. За обзором взаимодействий и потоков данных обращайтесь к разделу [«Взаимодействия и потоки данных»](architecture.md#2-взаимодействия-и-потоки-данных).

Общие архитектурные принципы

Микросервисный подход. Функциональные домены выделяются в самостоятельные сервисы с чёткими API-контрактами, управляемыми через Gateway или BFF (подробнее — [раздел 2.4 архитектуры](architecture.md#24-роль-gatewaybff)).

Единая база данных. Сервисы делят общий PostgreSQL-кластер, изолируясь на уровне схем, ролей и политик доступа; при существенном росте нагрузки возможен поэтапный переход к выделенным кластерам для отдельных доменов (см. [описание кластера](architecture.md#23-единый-postgresql-кластер)).

Асинхронные коммуникации. Для обмена событиями и фоновых задач применяются очереди сообщений (RabbitMQ) и публикация доменных событий; композицию потоков см. в [разделе 2.2 архитектуры](architecture.md#22-асинхронная-шина-rabbitmq).

Кеширование. Для ускорения чтения справочных и агрегированных данных используется Redis или аналогичный in-memory кеш с TTL и контролем согласованности.

Наблюдаемость и устойчивость. Все сервисы обязаны вести структурированные логи, метрики и трассировки (Prometheus, Grafana, OpenTelemetry), обеспечивать алерты и обработку ошибок с повторными попытками.

Управление конфигурацией и секретами. Конфигурации хранятся централизованно (Vault/SSM/Secrets Manager), секреты не попадают в репозиторий, развёртывание автоматизируется через CI/CD.

Безопасность по умолчанию. Реализованы единые механизмы аутентификации/авторизации, шифрование трафика, контроль доступа и регулярные проверки на соответствие требованиям безопасности.

Инфраструктура

Расширенный пример переменных окружения для локальной разработки и CI/CD собран в файле [`env.example`](../env.example). Он отражает секции «Базы данных», «Брокеры сообщений и кэши» и «Service discovery», а также интеграции (Google Drive, Telegram), описанные ниже.

Политика ведения seed-миграций и владение справочниками зафиксированы в разделе [«Ведение seed-миграций»](data-model.md#ведение-seed-миграций). Практические шаги по загрузке тестовых данных описаны в [docs/testing-data.md](testing-data.md).

Базы данных.

Основные сервисы (Auth, CRM/Deals, Payments, Tasks, Notifications) подключаются к единому PostgreSQL-кластеру. Изоляция достигается за счёт выделенных схем, отдельных ролей и политик row-level security там, где требуется. Кластер развёрнут в конфигурации primary–standby с резервным копированием в Backup-сервис. Для реактивных приложений (Auth, Payments, Audit) переменные окружения используют R2DBC URI, например `r2dbc:postgresql://auth:auth@localhost:5432/crm?schema=auth`; допускается эквивалент с `search_path=auth`.

Audit использует ту же инсталляцию PostgreSQL, но ведёт журналы в собственной схеме с включённым логированием длительных транзакций и повышенными SLA на хранение.

Брокеры сообщений и кэши.

RabbitMQ развёрнут в продакшене в кластере из трёх нод с quorum queue и mirrored classic queue для критичных очередей. Управление осуществляется через Kubernetes-оператор, конфигурации декларативно описаны в GitOps-репозитории. Для обеспечения надёжности включены автоматическое промоутирование лидеров, лимиты на TTL и длину очередей, а также политика dead-letter для повторяемых ошибок. Требования по мониторингу включают контроль глубины очередей, времени обработки сообщений и состояния потребителей; метрики собираются в Prometheus, алерты поддерживаются в Grafana, резервные копии конфигураций и политик выгружаются в Backup-сервис. Payments публикует доменные события в exchange payments.events, откуда Notifications, CRM/Deals и Tasks подписываются на соответствующие routing key.

Redis — общий высокодоступный кластер (Sentinel с не менее чем тремя узлами) для кэширования сессий Gateway, очередей Celery в CRM/Deals, задач BullMQ в Documents, а также FSM и rate limiting в Telegram Bot и Notifications. Для отказоустойчивости настроены автоматическое переключение мастера с подтверждением репликации, лимиты на нагрузку по базам Redis и отдельные namespace/DB для сервисов. Мониторинг покрывает задержки, использование памяти и размер очередей Celery/BullMQ, а также состояние воркеров; показатели агрегируются в Prometheus с алертами в Grafana и логированием критичных событий в Loki. Резервирование выполняется через периодические снапшоты RDB/AOF, выгружаемые в Backup-сервис вместе с метаданными Sentinel.

Service discovery.

Consul развёрнут в Kubernetes через официальный Helm-чарт в конфигурации из трёх серверов и набора клиентских агентов на каждом
узле.

* Развёртывание: серверные узлы размещаются в отдельном StatefulSet с persistent volume и автоматическими health-checks;
  клиентские агенты доставляются DaemonSet-ом с sidecar-проверками для сервисов Gateway и внутренних API.
* Мониторинг: Consul-агенты экспортируют метрики в Prometheus, дашборды и алерты поддерживаются в Grafana, а технические события
  и аудит ACL транслируются в Loki.
* Резервирование и восстановление: снапшоты состояния ключ-значение и сервисных конфигураций отправляются в Backup-сервис,
  дополнительно по расписанию выгружаются ACL и intentions. Для DR-сценариев описаны инструкции восстановления кворума из
  последнего консистентного снапшота.

Контейнеризация и оркестрация.

Для локальной разработки сервисы поднимаются через Docker Compose.

В тестовой среде используется k3s, в продакшене — управляемый Kubernetes-кластер с Helm-чартами и сетевой политикой через Istio.

Логирование и мониторинг.

Логи собираются через Promtail в Loki.

Метрики собирает Prometheus, визуализация и алерты настраиваются в Grafana.

Трейсинг ключевых операций реализован через OpenTelemetry и Tempo.

CI/CD и GitOps.

Исторически GitHub Actions запускал конвейер `ci.yml`: матричный запуск lint/unit/contract-тестов для Gateway (Node.js/pnpm) с заготовками под Gradle (Auth) и Poetry (CRM) сервисы, а также сборку и публикацию OCI-образов в GHCR с кэшированием Docker-слоёв (`cache-from/cache-to` на BuildKit). Сейчас файл переименован в `ci.yml.disabled`, поэтому пайплайн не стартует автоматически; повторяйте шаги локально и при необходимости верните расширение `.yml`.

Переменная `CI_REGISTRY_IMAGE` используется как базовый префикс образов (`${CI_REGISTRY_IMAGE}-gateway` и т.д.); пайплайн логинится в `ghcr.io` через `GITHUB_TOKEN` и повторно использует кэш pnpm/Buildx между прогоном lint/tests и сборкой.

Argo CD синхронизирует Kubernetes-кластер с каталогом `infra/k8s`: базовые манифесты (`namespace`, `Deployment`, `Service`, `ConfigMap`, `Secret`) и overlay (`overlays/dev|stage|prod`) описывают параметры Gateway и зависимостей (Redis), а Application-манифесты (`infra/k8s/argocd/gateway-apps.yaml`) подключают окружения с автоматической синхронизацией.

Интеграции
Google Drive

Авторизация по OAuth 2.0 с сервисным аккаунтом Google Workspace; ключи хранятся в Secrets Manager. Для локальной разработки используйте эмулятор из раздела [«Интеграции» в руководстве по локальной среде](local-setup.md#интеграции).

Documents-сервис управляет OAuth-токенами, получает доступ к корневой папке проекта и создаёт вложенные каталоги по структуре из раздела «Хранение документов» в README.

Метаданные файлов (ID Drive, ссылка, владельцы-сущности, хэш, автор, временные метки) сохраняются в PostgreSQL Documents-сервиса.

Telegram-бот

Бот принимает обновления через HTTPS-webhook, терминируемый в Gateway; используется обратный прокси с автоматическим обновлением TLS-сертификатов (Let’s Encrypt). Для локального тестирования webhook и Bot API подключайте mock-сервер по инструкции из [docs/local-setup.md#интеграции](local-setup.md#интеграции).

Команды и уведомления помещаются в очередь RabbitMQ, откуда их читают Notifications и CRM/Deals. Ответы пользователям отправляются асинхронно.

Привязка Telegram-пользователя к учётной записи CRM хранится в Auth-сервисе и выполняется через REST-вызовы шлюза `/api/auth/telegram/*`.

Политика хранения импортируемых файлов

Файлы загружаются через Documents-сервис напрямую в Google Drive; в PostgreSQL сохраняется только метаинформация (ID, ссылки, привязки к сущностям), а бинарные данные в CRM не хранятся.

Служебные файлы, прошедшие импорт, помечаются TTL в 90 дней; по истечении срока Documents-сервис инициирует проверку продления и при отсутствии подтверждения удаляет их из Drive.

Метаданные и события импорта фиксируются в Audit и сохраняются бессрочно для восстановления истории действий.

Стек по сервисам

Фронтенд

Ядро: React 18 + TypeScript, собранные поверх Next.js 14 в режиме App Router для гибридного SSR/SSG и оптимизаций по производительности.

Сборка и поставка: встроенный bundler Next.js (SWC) и Turbopack в режиме разработки; артефакт собирается в Docker-образ, который проходит через pipeline GitHub Actions → GitHub Container Registry → Argo CD для выката в Kubernetes (отдельный deployment `frontend`).

Управление состоянием: React Query для работы с асинхронными данными Gateway/BFF, Zustand для локального UI-состояния и кэширования пользовательских настроек, Context API для тем и параметров локализации.

Взаимодействие с Gateway/BFF: REST-запросы через обёртку вокруг `fetch` с автоматическим проставлением токена сессии (httpOnly cookie); в первой поставке фронтенд открывает только три SSE-канала — сделки (`deals`), платежи (`payments`) и внутренние уведомления (`notifications`). Все вызовы проходят через `/api` шлюз, домен и адреса стриминговых каналов задаются переменными окружения (`NEXT_PUBLIC_CRM_SSE_URL`, `NEXT_PUBLIC_PAYMENTS_SSE_URL`, `NEXT_PUBLIC_NOTIFICATIONS_SSE_URL`). Значения по умолчанию в [`env.example`](../env.example) указывают на документированные маршруты Gateway — `https://gateway.local/api/v1/streams/deals`, `https://gateway.local/api/v1/streams/payments` и `https://gateway.local/api/v1/streams/notifications` соответственно. Внутренний upstream для CRM продолжает обозначаться как `crm`, однако публичный маршрут и фронтенд-конфигурация используют имя `deals` для единообразия. Отдельный поток задач появится после расширения сценариев напоминаний; в текущем релизе соединение с Tasks не открывается.

Тестирование: unit и компонентные тесты на Vitest + React Testing Library, визуальные снапшоты Storybook Chromatic, end-to-end сценарии в Playwright. Smoke-тесты фронтенда запускаются после деплоя вместе с контрактными тестами Gateway/BFF.

Требования к окружению:

* `NEXT_PUBLIC_API_BASE_URL` — публичный URL Gateway/BFF, используемый при серверном рендеринге и на клиенте (локально по умолчанию `http://localhost:8080/api`).
* `NEXT_PUBLIC_TELEMETRY_DSN` — DSN для фронтенд-логирования/трейсинга (Sentry или аналог), передаётся в runtime.
* `NEXT_PUBLIC_FEATURE_FLAGS` — перечисление включённых feature-флагов (через запятую), синхронизировано с LaunchDarkly/ConfigCat.
* `FRONTEND_PROXY_TIMEOUT` — таймаут проксирования на уровне Next.js middleware для долгих запросов.

Связь с инфраструктурой: переменные окружения подставляются через Secrets Manager → GitHub Actions → Kubernetes secrets; окружения `dev/stage/prod` получают собственные конфигурации API и ключей телеметрии. Проверка соответствия схем API и e2e-сценариев выполняется в CI перед синхронизацией Argo CD. Для локального запуска значения считываются из файла `.env.local`, основанного на [`env.example`](../env.example) в корне репозитория.

Gateway / BFF

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS с модулем @nestjs/axios для агрегации

БД и очереди: ioredis (Redis); прямой работы с PostgreSQL нет

API: REST (JSON) и SSE-каналы (прокси потоков CRM/Deals, Payments и Notifications; отдельный канал Tasks отложен до следующих релизов), внутренние вызовы — REST и управление стримами

Зависимости и компоненты:

Redis для сессий и кеша

Service Discovery через Consul

@nestjs/platform-sse для серверных событий и `eventsource-parser`/RxJS для ретрансляции потоков SSE

Тестирование и деплой:

Контрактные тесты и smoke-тесты UI

Blue/green деплой без простоев

Auth

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Security)

БД и очереди: R2DBC (PostgreSQL), lettuce (Redis)

API: REST + OAuth2/OpenID Connect

Зависимости:

PostgreSQL-схема auth

Redis для токенов и кодов

Notifications/Telegram Bot для выдачи одноразовых подтверждений входа, активации новых аккаунтов и быстрой валидации операций

CRM/Deals инициирует приглашение пользователя, а Telegram Bot доставляет одноразовый токен активации; подтверждение учётной записи происходит в Auth по предъявлению токена

Внутренний сервис администрирования для генерации стартовых токенов и восстановления доступа через CRM/Telegram с протоколированием операций в Audit

Тестирование и деплой:

JUnit5 + Testcontainers

Миграции Liquibase, ротация JWT-ключей

CRM / Deals

Язык: Python 3.11

Фреймворк: FastAPI

БД и очереди: SQLAlchemy 2.0 + Alembic (PostgreSQL), Celery + Redis, RabbitMQ (aio-pika)

API: REST + SSE (через `sse-starlette` для стриминга событий сделок)

Зависимости:

PostgreSQL-схема crm

Redis (очередь Celery)

Использует общий кластер Redis, описанный в разделе «Брокеры сообщений и кэши», и разделяет мониторинг очередей Celery с Notifications.

RabbitMQ выступает шиной доменных событий: сервис подписывается на exchange `payments.events`, читая сообщения через выделенную очередь `crm.payments-sync` с ключами `payments.*`. Для публикации собственных событий (`deal.created`, `deal.updated`, `task.assigned`, документы; SLA-события добавятся после Этапа 1.1) используется отдельный exchange `crm.events`, привязанный к очередям Notifications и Tasks (см. [архитектуру, раздел 2.2](architecture.md#22-асинхронная-шина-rabbitmq)). Подключение реализовано поверх `aio-pika` с подтверждением доставки (publisher confirms) и ручным ack со стороны консьюмера.

Повторные попытки организованы через пару `crm.payments-sync.retry`/`crm.payments-sync.dlx`: основная очередь настроена на dead-letter при ошибках обработки, сообщения попадают в retry-очередь с TTL (60 секунд по умолчанию) и возвращаются в рабочую очередь ограниченное число раз. Невосстановимые сообщения сохраняются в `crm.payments-sync.dlx` для ручного анализа и корреляции с Audit.

Интеграция с Documents-сервисом: загрузки и скачивания проходят через его API, который создаёт и управляет файлами в Google Drive; в PostgreSQL CRM хранит только метаданные (ID, ссылки, привязку к сущностям).

Тестирование и деплой:

Pytest + async-интеграции

Alembic миграции, прогрев кеша

Payments

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Spring Cloud Stream)

БД и очереди: Spring Data R2DBC (PostgreSQL), Spring AMQP (RabbitMQ)

API: REST, публикация событий в RabbitMQ

Зависимости:

PostgreSQL-схема payments

RabbitMQ exchange payments.events с маршрутизацией по ключам операций (создание, изменение статусов, возвраты)

Внутренние справочники тарифов CRM с фиксированной валютой учёта RUB без дополнительных пересчётов

Особенности работы
------------------

* Все расчёты ведутся в базовой валюте CRM (RUB) без интеграции с внешними сервисами конвертации.
* Gateway предоставляет REST-эндпоинты Payments только для внутренних клиентов (`/api/v1/payments/**`).

Обмен событиями:

* сервис создаёт сообщения в формате CloudEvents и публикует их в exchange payments.events через Spring AMQP с подтверждением доставки (publisher confirms);
* Notifications, CRM/Deals и Tasks потребляют события через подписки на очереди с собственными routing key и поддержкой повторных попыток через dead-letter;
* Audit фиксирует критичные события из отдельной очереди с выдержанным SLA на доставку.

Тестирование и деплой:

JUnit5 + Testcontainers для PostgreSQL и RabbitMQ

Миграции Flyway выполняются из Spring Boot при старте и в CI/CD; конфигурация exchange, очередей и биндингов описана декларативно через Spring AMQP (RabbitAdmin) и применяется вместе с релизом; поэтапный rolling update

Documents

Язык: TypeScript (Node.js)

Фреймворк: NestJS + @googleapis/drive SDK

БД и очереди: TypeORM (PostgreSQL), BullMQ (Redis)

API: REST + Webhook

Зависимости:

PostgreSQL-схема documents

Redis кластер для синхронизации

Очереди BullMQ обслуживаются тем же высокодоступным Redis, что и Celery; требования к отказоустойчивости и мониторингу см. в разделе «Брокеры сообщений и кэши».

Общие сервисные аккаунты Google Drive

Тестирование и деплой:

Интеграционные тесты с песочницей Drive

Проверка квот API, миграции TypeORM

Tasks и Notifications могут развёртываться в одной инфраструктурной связке (общий репозиторий, пайплайн, shared-модули NestJS),
но остаются отдельными сервисами с собственными схемами БД и очередями. Ниже приведены их стек и зависимости.

Tasks

Первая поставка ограничивается ручным созданием и назначением задач без SLA-таймеров, повторяющихся задач и cron-триггеров; эти возможности запланированы на [Этап 1.1](delivery-plan.md#2-приоритизация-последующих-этаов).

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS (@nestjs/schedule, CQRS-модули для команд и событий)

БД и очереди: TypeORM (PostgreSQL, схема `tasks`), @golevelup/nestjs-rabbitmq (RabbitMQ), BullMQ (Redis) для отложенных задач SLA (активируются с Этапа 1.1)

API: REST (управление задачами) + внутренние webhook-и для уведомлений и подтверждений (поддержка SLA и повторов появится после Этапа 1.1)

Зависимости:

PostgreSQL-схема tasks

RabbitMQ очереди `tasks.command` и `tasks.events` (подписка на `payments.events.*`, `crm.deal.*`)

Redis (ioredis) для краткоживущих таймеров и блокировок повторного запуска

Notifications API для триггеров напоминаний

Тестирование и деплой:

Jest + supertest, потребительские контракты RabbitMQ, e2e-сценарии с Testcontainers

TypeORM миграции, canary-релизы с прогревом очередей

Notifications

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS (@nestjs/platform-sse, @nestjs/event-emitter)

БД и очереди: TypeORM (PostgreSQL, схема `notifications`), @golevelup/nestjs-rabbitmq (RabbitMQ)

API: REST + SSE (однонаправленные каналы уведомлений), публикация уведомлений в RabbitMQ и webhook-и в Gateway/Telegram

Зависимости:

PostgreSQL-схема notifications

RabbitMQ exchange `notifications.events`, очереди для Telegram-бота и внутреннего SSE-канала CRM

Redis (ioredis) для rate limiting и хранения одноразовых токенов подтверждения

Gateway для маршрутизации внешних webhook-ов Telegram, Tasks для статусов напоминаний

Тестирование и деплой:

Jest + Pact (контракты на очереди и webhook-и), нагрузочные тесты SSE-каналов

TypeORM миграции, canary-релизы с мониторингом доставки сообщений

Telegram Bot

Язык: Python 3.11

Фреймворк: aiogram 3

БД и очереди: asyncpg (PostgreSQL), aio-pika (RabbitMQ)

API: Вебхуки Telegram + внутренний REST Callback

Зависимости:

PostgreSQL-схема bot (readonly к auth и crm)

Очередь notifications.telegram

Redis для FSM и rate limiting

Работает поверх общего Redis-кластера с Sentinel, покрытого мониторингом и бэкапами, описанными в разделе «Брокеры сообщений и кэши».

Тестирование и деплой:

Pytest-asyncio + моки Telegram API

End-to-end сценарии в staging, blue/green деплой

<a id="audit"></a>Audit

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Actuator, Spring Cloud Stream)

БД и очереди: Spring Data R2DBC (PostgreSQL), Spring AMQP (RabbitMQ)

API: REST endpoints для внутренних подписчиков; события доставляются через Spring Cloud Stream

Интеграция с PostgreSQL:

* журнальные таблицы разбиваются на ежемесячные партиции внутри схемы audit, включён контроль долгих транзакций и ретеншн на уровне политик;
* запись событий выполняется через batch-вставки с подтверждением и трекингом idempotency key на основе reactive R2DBC-транзакций;
* ежедневная агрегация метрик действий пользователей формируется через Spring Batch и записывается в материализованные представления, которые потребляются Reports и внутренними дашбордами.

Интеграция с RabbitMQ:

* сервис подписывается на exchange audit.events и очередь audit.core с quorum queue; сообщения публикуют Payments, Auth, CRM/Deals и Notifications;
* включены ручные подтверждения доставки, дедупликация по message-id и сохранение dead-letter событий в отдельной очереди audit.dlq с мониторингом через Spring AMQP;
* при сбое основного хранилища события буферизуются в локальном write-ahead журнале на PersistentVolume до восстановления соединения с PostgreSQL.

Тестирование и деплой:

* Интеграционные тесты на JUnit5 + Testcontainers (PostgreSQL, RabbitMQ);
* Развёртывание через Kubernetes StatefulSet с подстроенными ресурсными квотами, rolling update с прогревом кэша справочников;
* Конфигурация очередей описывается в Helm-чарте и синхронизируется Argo CD; миграции схемы управляются через Liquibase в CI/CD.

<a id="backup"></a>Backup

Язык: Python 3.11

Фреймворк: FastAPI + APScheduler

БД и очереди: psycopg (PostgreSQL), boto3 (S3-совместимое хранилище), aio-pika (RabbitMQ для уведомлений)

API: REST для управления бэкапами, health-check реализован через REST endpoint `/health` и метрики Prometheus

Механизмы резервного копирования:

* базы PostgreSQL выгружаются через `pg_dump` (инкрементальные схемные бэкапы) и `pg_basebackup` для полного восстановления; артефакты сжимаются и отправляются в версионируемое S3-хранилище;
* Consul снапшоты получаются через `consul snapshot save`, RabbitMQ конфигурации и политики выгружаются с помощью `rabbitmqadmin export` и хранятся вместе с базовыми бэкапами;
* Redis снапшоты RDB/AOF, включая базы Celery и BullMQ, выполняются с контролем успешной репликации Sentinel и загружаются в то же S3-хранилище, что и другие инфраструктурные бэкапы;
* журналы выполнения задач и контрольные суммы артефактов пишутся в таблицу backup.jobs, что позволяет перезапускать операции и отслеживать SLA.

Оркестрация и уведомления:

* APScheduler формирует расписания ежедневных, еженедельных и внеочередных бэкапов;
* статусы задач публикуются в очередь backup.notifications (RabbitMQ), откуда Notifications отправляет предупреждения дежурной команде;
* S3-хранилище монтируется через CSI-драйвер, секреты доступа подтягиваются из Vault;
* Kubernetes выполняет liveness/readiness-пробы по `GET /healthz`, а результаты проб фиксируются в метриках наблюдаемости.

Тестирование и деплой:

* pytest + moto для имитации S3, Testcontainers для PostgreSQL и RabbitMQ, отдельные smoke-тесты вызывают `GET /healthz` после выката;
* деплой как Kubernetes CronJob + вспомогательный Deployment для REST-API управления, использование init-контейнеров для проверки доступности хранилищ и автоматическая проверка `/healthz` перед переводом трафика;
* политики ретенции управляются через Helm values и проверяются линтером kube-score в CI.

Дальнейшее развитие

Список технологий будет расширяться по мере детализации сервисов и инфраструктуры (СУБД, брокеры, деплой, интеграции).
Все изменения документируются в docs/ и проходят архитектурное ревью.