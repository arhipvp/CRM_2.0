Назначение документа

Документ фиксирует базовые архитектурные принципы и технологические решения CRM-системы.
Он служит отправной точкой для команд разработки, эксплуатации и бизнес-заказчиков, помогая синхронизировать ожидания относительно сервисов и инфраструктуры.
Материал дополняет обзорные документы (README.md, docs/architecture.md, docs/security-and-access.md) и задаёт общие правила для детальных спецификаций отдельных сервисов. За обзором взаимодействий и потоков данных обращайтесь к разделу [«Взаимодействия и потоки данных»](docs/architecture.md#2-взаимодействия-и-потоки-данных).

Общие архитектурные принципы

Микросервисный подход. Функциональные домены выделяются в самостоятельные сервисы с чёткими API-контрактами, управляемыми через Gateway или BFF (подробнее — [раздел 2.4 архитектуры](docs/architecture.md#24-роль-gatewaybff)).

Единая база данных. Сервисы делят общий PostgreSQL-кластер, изолируясь на уровне схем, ролей и политик доступа; при существенном росте нагрузки возможен поэтапный переход к выделенным кластерам для отдельных доменов (см. [описание кластера](docs/architecture.md#23-единый-postgresql-кластер)).

Асинхронные коммуникации. Для обмена событиями и фоновых задач применяются очереди сообщений (RabbitMQ) и публикация доменных событий; композицию потоков см. в [разделе 2.2 архитектуры](docs/architecture.md#22-асинхронная-шина-rabbitmq).

Кеширование. Для ускорения чтения справочных и агрегированных данных используется Redis или аналогичный in-memory кеш с TTL и контролем согласованности.

Наблюдаемость и устойчивость. Все сервисы обязаны вести структурированные логи, метрики и трассировки (Prometheus, Grafana, OpenTelemetry), обеспечивать алерты и обработку ошибок с повторными попытками.

Управление конфигурацией и секретами. Конфигурации хранятся централизованно (Vault/SSM/Secrets Manager), секреты не попадают в репозиторий, развёртывание автоматизируется через CI/CD.

Безопасность по умолчанию. Реализованы единые механизмы аутентификации/авторизации, шифрование трафика, контроль доступа и регулярные проверки на соответствие требованиям безопасности.

Инфраструктура

Расширенный пример переменных окружения для локальной разработки и CI/CD собран в файле [`env.example`](../env.example). Он отражает секции «Базы данных», «Брокеры сообщений и кэши» и «Service discovery», а также интеграции (Google Drive, Telegram), описанные ниже.

Базы данных.

Основные сервисы (Auth, CRM/Deals, Payments, Tasks, Notifications) подключаются к единому PostgreSQL-кластеру. Изоляция достигается за счёт выделенных схем, отдельных ролей и политик row-level security там, где требуется. Кластер развёрнут в конфигурации primary–standby с резервным копированием в Backup-сервис.

Audit использует ту же инсталляцию PostgreSQL, но ведёт журналы в собственной схеме с включённым логированием длительных транзакций и повышенными SLA на хранение.

Брокеры сообщений и кэши.

RabbitMQ развёрнут в продакшене в кластере из трёх нод с quorum queue и mirrored classic queue для критичных очередей. Управление осуществляется через Kubernetes-оператор, конфигурации декларативно описаны в GitOps-репозитории. Для обеспечения надёжности включены автоматическое промоутирование лидеров, лимиты на TTL и длину очередей, а также политика dead-letter для повторяемых ошибок. Требования по мониторингу включают контроль глубины очередей, времени обработки сообщений и состояния потребителей; метрики собираются в Prometheus, алерты поддерживаются в Grafana, резервные копии конфигураций и политик выгружаются в Backup-сервис. Payments публикует доменные события в exchange payments.events, откуда Notifications, CRM/Deals и Tasks подписываются на соответствующие routing key.

Redis — общий высокодоступный кластер (Sentinel с не менее чем тремя узлами) для кэширования сессий Gateway, очередей Celery в CRM/Deals, задач BullMQ в Documents, а также FSM и rate limiting в Telegram Bot и Notifications. Для отказоустойчивости настроены автоматическое переключение мастера с подтверждением репликации, лимиты на нагрузку по базам Redis и отдельные namespace/DB для сервисов. Мониторинг покрывает задержки, использование памяти и размер очередей Celery/BullMQ, а также состояние воркеров; показатели агрегируются в Prometheus с алертами в Grafana и логированием критичных событий в Loki. Резервирование выполняется через периодические снапшоты RDB/AOF, выгружаемые в Backup-сервис вместе с метаданными Sentinel.

Service discovery.

Consul развёрнут в Kubernetes через официальный Helm-чарт в конфигурации из трёх серверов и набора клиентских агентов на каждом
узле.

* Развёртывание: серверные узлы размещаются в отдельном StatefulSet с persistent volume и автоматическими health-checks;
  клиентские агенты доставляются DaemonSet-ом с sidecar-проверками для сервисов Gateway и внутренних API.
* Мониторинг: Consul-агенты экспортируют метрики в Prometheus, дашборды и алерты поддерживаются в Grafana, а технические события
  и аудит ACL транслируются в Loki.
* Резервирование и восстановление: снапшоты состояния ключ-значение и сервисных конфигураций отправляются в Backup-сервис,
  дополнительно по расписанию выгружаются ACL и intentions. Для DR-сценариев описаны инструкции восстановления кворума из
  последнего консистентного снапшота.

Контейнеризация и оркестрация.

Для локальной разработки сервисы поднимаются через Docker Compose.

В тестовой среде используется k3s, в продакшене — управляемый Kubernetes-кластер с Helm-чартами и сетевой политикой через Istio.

Логирование и мониторинг.

Логи собираются через Promtail в Loki.

Метрики собирает Prometheus, визуализация и алерты настраиваются в Grafana.

Трейсинг ключевых операций реализован через OpenTelemetry и Tempo.

CI/CD и GitOps.

GitHub Actions выполняет сборку, тесты и публикацию контейнеров в GitHub Container Registry.

Argo CD синхронизирует Kubernetes-кластер с GitOps-репозиторием инфраструктуры после автоматических проверок качества.

Интеграции
Google Drive

Авторизация по OAuth 2.0 с сервисным аккаунтом Google Workspace; ключи хранятся в Secrets Manager.

Documents-сервис управляет OAuth-токенами, получает доступ к корневой папке проекта и создаёт вложенные каталоги по структуре из раздела «Хранение документов» в README.

Метаданные файлов (ID Drive, ссылка, владельцы-сущности, хэш, автор, временные метки) сохраняются в PostgreSQL Documents-сервиса.

Telegram-бот

Бот принимает обновления через HTTPS-webhook, терминируемый в Gateway; используется обратный прокси с автоматическим обновлением TLS-сертификатов (Let’s Encrypt).

Команды и уведомления помещаются в очередь RabbitMQ, откуда их читают Notifications и CRM/Deals. Ответы пользователям отправляются асинхронно.

Привязка Telegram-пользователя к учётной записи CRM хранится в Auth-сервисе и выполняется через REST-вызовы шлюза `/api/auth/telegram/*`.

Политика хранения импортируемых файлов

Файлы загружаются через Documents-сервис напрямую в Google Drive; в PostgreSQL сохраняется только метаинформация (ID, ссылки, привязки к сущностям), а бинарные данные в CRM не хранятся.

Служебные файлы, прошедшие импорт, помечаются TTL в 90 дней; по истечении срока Documents-сервис инициирует проверку продления и при отсутствии подтверждения удаляет их из Drive.

Метаданные и события импорта фиксируются в Audit и сохраняются бессрочно для восстановления истории действий.

Стек по сервисам

Фронтенд

Ядро: React 18 + TypeScript, собранные поверх Next.js 14 в режиме App Router для гибридного SSR/SSG и оптимизаций по производительности.

Сборка и поставка: встроенный bundler Next.js (SWC) и Turbopack в режиме разработки; артефакт собирается в Docker-образ, который проходит через pipeline GitHub Actions → GitHub Container Registry → Argo CD для выката в Kubernetes (отдельный deployment `frontend`).

Управление состоянием: React Query для работы с асинхронными данными Gateway/BFF, Zustand для локального UI-состояния и кэширования пользовательских настроек, Context API для тем и параметров локализации.

Взаимодействие с Gateway/BFF: REST-запросы через обёртку вокруг `fetch` с автоматическим проставлением токена сессии (httpOnly cookie); подключены SSE-каналы для обновлений статусов задач и платежей, которые транслирует Gateway из RabbitMQ. Все вызовы проходят через `/api` шлюз, домен которого задаётся переменными окружения.

Тестирование: unit и компонентные тесты на Vitest + React Testing Library, визуальные снапшоты Storybook Chromatic, end-to-end сценарии в Playwright. Smoke-тесты фронтенда запускаются после деплоя вместе с контрактными тестами Gateway/BFF.

Требования к окружению:

* `NEXT_PUBLIC_GATEWAY_BASE_URL` — публичный URL Gateway/BFF, используемый при серверном рендеринге и на клиенте.
* `NEXT_PUBLIC_TELEMETRY_DSN` — DSN для фронтенд-логирования/трейсинга (Sentry или аналог), передаётся в runtime.
* `NEXT_PUBLIC_FEATURE_FLAGS` — перечисление включённых feature-флагов (через запятую), синхронизировано с LaunchDarkly/ConfigCat.
* `FRONTEND_PROXY_TIMEOUT` — таймаут проксирования на уровне Next.js middleware для долгих запросов.

Связь с инфраструктурой: переменные окружения подставляются через Secrets Manager → GitHub Actions → Kubernetes secrets; окружения `dev/stage/prod` получают собственные конфигурации API и ключей телеметрии. Проверка соответствия схем API и e2e-сценариев выполняется в CI перед синхронизацией Argo CD. Для локального запуска значения считываются из файла `.env.local`, основанного на [`env.example`](../env.example) в корне репозитория.

Gateway / BFF

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS с модулем @nestjs/axios для агрегации

БД и очереди: ioredis (Redis); прямой работы с PostgreSQL нет

API: REST (JSON) и SSE-каналы для фронтенда, внутренние вызовы — REST и управление SSE-потоками

Зависимости и компоненты:

Redis для сессий и кеша

Service Discovery через Consul

Тестирование и деплой:

Контрактные тесты и smoke-тесты UI

Blue/green деплой без простоев

Auth

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Security)

БД и очереди: R2DBC (PostgreSQL), lettuce (Redis)

API: REST + OAuth2/OpenID Connect

Зависимости:

PostgreSQL-схема auth

Redis для токенов и кодов

Notifications/Telegram Bot для выдачи одноразовых подтверждений входа, активации новых аккаунтов и быстрой валидации операций

CRM/Deals инициирует приглашение пользователя, а Telegram Bot доставляет одноразовый токен активации; подтверждение учётной записи происходит в Auth по предъявлению токена

Внутренний сервис администрирования для генерации стартовых токенов и восстановления доступа через CRM/Telegram с протоколированием операций в Audit

Тестирование и деплой:

JUnit5 + Testcontainers

Миграции Liquibase, ротация JWT-ключей

CRM / Deals

Язык: Python 3.11

Фреймворк: FastAPI

БД и очереди: SQLAlchemy 2.0 + Alembic (PostgreSQL), Celery + Redis

API: REST + WebSocket

Зависимости:

PostgreSQL-схема crm

Redis (очередь Celery)

Использует общий кластер Redis, описанный в разделе «Брокеры сообщений и кэши», и разделяет мониторинг очередей Celery с Notifications.

Интеграция с Documents-сервисом: загрузки и скачивания проходят через его API, который создаёт и управляет файлами в Google Drive; в PostgreSQL CRM хранит только метаданные (ID, ссылки, привязку к сущностям).

Тестирование и деплой:

Pytest + async-интеграции

Alembic миграции, прогрев кеша

Payments

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Spring Cloud Stream)

БД и очереди: Spring Data R2DBC (PostgreSQL), Spring AMQP (RabbitMQ)

API: REST, публикация событий в RabbitMQ

Зависимости:

PostgreSQL-схема payments

RabbitMQ exchange payments.events с маршрутизацией по ключам операций (создание, изменение статусов, возвраты)

Внешний API валютных курсов через Gateway

Обмен событиями:

* сервис создаёт сообщения в формате CloudEvents и публикует их в exchange payments.events через Spring AMQP с подтверждением доставки (publisher confirms);
* Notifications, CRM/Deals и Tasks потребляют события через подписки на очереди с собственными routing key и поддержкой повторных попыток через dead-letter;
* Audit фиксирует критичные события из отдельной очереди с выдержанным SLA на доставку.

Тестирование и деплой:

JUnit5 + Testcontainers для PostgreSQL и RabbitMQ

Миграции Flyway выполняются из Spring Boot при старте и в CI/CD; конфигурация exchange, очередей и биндингов описана декларативно через Spring AMQP (RabbitAdmin) и применяется вместе с релизом; поэтапный rolling update

Documents

Язык: TypeScript (Node.js)

Фреймворк: NestJS + @googleapis/drive SDK

БД и очереди: TypeORM (PostgreSQL), BullMQ (Redis)

API: REST + Webhook

Зависимости:

PostgreSQL-схема documents

Redis кластер для синхронизации

Очереди BullMQ обслуживаются тем же высокодоступным Redis, что и Celery; требования к отказоустойчивости и мониторингу см. в разделе «Брокеры сообщений и кэши».

Общие сервисные аккаунты Google Drive

Тестирование и деплой:

Интеграционные тесты с песочницей Drive

Проверка квот API, миграции TypeORM

Tasks и Notifications могут развёртываться в одной инфраструктурной связке (общий репозиторий, пайплайн, shared-модули NestJS),
но остаются отдельными сервисами с собственными схемами БД и очередями. Ниже приведены их стек и зависимости.

Tasks

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS (@nestjs/schedule, CQRS-модули для команд и событий)

БД и очереди: TypeORM (PostgreSQL, схема `tasks`), @golevelup/nestjs-rabbitmq (RabbitMQ), BullMQ (Redis) для отложенных задач SLA

API: REST (управление задачами) + внутренние webhook-и для уведомлений и подтверждений

Зависимости:

PostgreSQL-схема tasks

RabbitMQ очереди `tasks.command` и `tasks.events` (подписка на `payments.events.*`, `crm.deal.*`)

Redis (ioredis) для краткоживущих таймеров и блокировок повторного запуска

Notifications API для триггеров напоминаний

Тестирование и деплой:

Jest + supertest, потребительские контракты RabbitMQ, e2e-сценарии с Testcontainers

TypeORM миграции, canary-релизы с прогревом очередей

Notifications

Язык: TypeScript (Node.js LTS)

Фреймворк: NestJS (@nestjs/websockets, @nestjs/event-emitter)

БД и очереди: TypeORM (PostgreSQL, схема `notifications`), @golevelup/nestjs-rabbitmq (RabbitMQ)

API: REST + WebSocket, публикация уведомлений в RabbitMQ и webhook-и в Gateway/Telegram

Зависимости:

PostgreSQL-схема notifications

RabbitMQ exchange `notifications.events`, очереди для Telegram-бота и внутренних каналов CRM

Redis (ioredis) для rate limiting и хранения одноразовых токенов подтверждения

Gateway для маршрутизации внешних webhook-ов Telegram, Tasks для статусов напоминаний

Тестирование и деплой:

Jest + Pact (контракты на очереди и webhook-и), нагрузочные тесты WebSocket каналов

TypeORM миграции, canary-релизы с мониторингом доставки сообщений

Telegram Bot

Язык: Python 3.11

Фреймворк: aiogram 3

БД и очереди: asyncpg (PostgreSQL), aio-pika (RabbitMQ)

API: Вебхуки Telegram + внутренний REST Callback

Зависимости:

PostgreSQL-схема bot (readonly к auth и crm)

Очередь notifications.telegram

Redis для FSM и rate limiting

Работает поверх общего Redis-кластера с Sentinel, покрытого мониторингом и бэкапами, описанными в разделе «Брокеры сообщений и кэши».

Тестирование и деплой:

Pytest-asyncio + моки Telegram API

End-to-end сценарии в staging, blue/green деплой

<a id="audit"></a>Audit

Язык: Kotlin (JVM 17)

Фреймворк: Spring Boot (WebFlux + Actuator, Spring Cloud Stream)

БД и очереди: Spring Data R2DBC (PostgreSQL), Spring AMQP (RabbitMQ)

API: REST endpoints для внутренних подписчиков; события доставляются через Spring Cloud Stream

Интеграция с PostgreSQL:

* журнальные таблицы разбиваются на ежемесячные партиции внутри схемы audit, включён контроль долгих транзакций и ретеншн на уровне политик;
* запись событий выполняется через batch-вставки с подтверждением и трекингом idempotency key на основе reactive R2DBC-транзакций;
* ежедневная агрегация метрик действий пользователей формируется через Spring Batch и записывается в материализованные представления, которые потребляются Reports и внутренними дашбордами.

Интеграция с RabbitMQ:

* сервис подписывается на exchange audit.events и очередь audit.core с quorum queue; сообщения публикуют Payments, Auth, CRM/Deals и Notifications;
* включены ручные подтверждения доставки, дедупликация по message-id и сохранение dead-letter событий в отдельной очереди audit.dlq с мониторингом через Spring AMQP;
* при сбое основного хранилища события буферизуются в локальном write-ahead журнале на PersistentVolume до восстановления соединения с PostgreSQL.

Тестирование и деплой:

* Интеграционные тесты на JUnit5 + Testcontainers (PostgreSQL, RabbitMQ);
* Развёртывание через Kubernetes StatefulSet с подстроенными ресурсными квотами, rolling update с прогревом кэша справочников;
* Конфигурация очередей описывается в Helm-чарте и синхронизируется Argo CD; миграции схемы управляются через Liquibase в CI/CD.

<a id="backup"></a>Backup

Язык: Python 3.11

Фреймворк: FastAPI + APScheduler

БД и очереди: psycopg (PostgreSQL), boto3 (S3-совместимое хранилище), aio-pika (RabbitMQ для уведомлений)

API: REST для управления бэкапами, health-check реализован через REST endpoint `/health` и метрики Prometheus

Механизмы резервного копирования:

* базы PostgreSQL выгружаются через `pg_dump` (инкрементальные схемные бэкапы) и `pg_basebackup` для полного восстановления; артефакты сжимаются и отправляются в версионируемое S3-хранилище;
* Consul снапшоты получаются через `consul snapshot save`, RabbitMQ конфигурации и политики выгружаются с помощью `rabbitmqadmin export` и хранятся вместе с базовыми бэкапами;
* Redis снапшоты RDB/AOF, включая базы Celery и BullMQ, выполняются с контролем успешной репликации Sentinel и загружаются в то же S3-хранилище, что и другие инфраструктурные бэкапы;
* журналы выполнения задач и контрольные суммы артефактов пишутся в таблицу backup.jobs, что позволяет перезапускать операции и отслеживать SLA.

Оркестрация и уведомления:

* APScheduler формирует расписания ежедневных, еженедельных и внеочередных бэкапов;
* статусы задач публикуются в очередь backup.notifications (RabbitMQ), откуда Notifications отправляет предупреждения дежурной команде;
* S3-хранилище монтируется через CSI-драйвер, секреты доступа подтягиваются из Vault.

Тестирование и деплой:

* pytest + moto для имитации S3, Testcontainers для PostgreSQL и RabbitMQ;
* деплой как Kubernetes CronJob + вспомогательный Deployment для REST-API управления, использование init-контейнеров для проверки доступности хранилищ;
* политики ретенции управляются через Helm values и проверяются линтером kube-score в CI.

Дальнейшее развитие

Список технологий будет расширяться по мере детализации сервисов и инфраструктуры (СУБД, брокеры, деплой, интеграции).
Все изменения документируются в docs/ и проходят архитектурное ревью.